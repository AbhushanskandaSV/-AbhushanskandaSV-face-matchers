import cv2
import numpy as np
import sys
import time
import mediapipe as mp

def test_camera_access():
    """Test different camera indices to find working camera"""
    print("Testing camera access...")
    
    for i in range(5):  # Test camera indices 0-4
        print(f"Testing camera index {i}...")
        cap = cv2.VideoCapture(i)
        
        if cap.isOpened():
            ret, frame = cap.read()
            if ret and frame is not None:
                print(f"âœ“ Camera {i} is working!")
                cap.release()
                return i
            else:
                print(f"âœ— Camera {i} opened but couldn't read frame")
        else:
            print(f"âœ— Camera {i} failed to open")
        
        cap.release()
    
    print("No working camera found!")
    return None

def initialize_cascades():
    """Initialize cascade classifiers with error checking"""
    try:
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
        smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')
        
        # Check if cascades loaded properly
        if face_cascade.empty() or eye_cascade.empty() or smile_cascade.empty():
            print("Error: Could not load cascade classifiers!")
            return None, None, None
            
        print("âœ“ All cascade classifiers loaded successfully")
        return face_cascade, eye_cascade, smile_cascade
        
    except Exception as e:
        print(f"Error loading cascades: {e}")
        return None, None, None

def initialize_mediapipe():
    """Initialize MediaPipe hands solution"""
    try:
        mp_hands = mp.solutions.hands
        hands = mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )
        mp_drawing = mp.solutions.drawing_utils
        print("âœ“ MediaPipe hands initialized successfully")
        return hands, mp_hands, mp_drawing
    except Exception as e:
        print(f"Error initializing MediaPipe: {e}")
        return None, None, None

def detect_hand_gesture(landmarks):
    """Detect hand gestures based on landmark positions"""
    if not landmarks:
        return "No Hand", (128, 128, 128)
    
    # Get landmark positions
    thumb_tip = landmarks[4]
    thumb_ip = landmarks[3]
    
    index_tip = landmarks[8]
    index_pip = landmarks[6]
    index_mcp = landmarks[5]
    
    middle_tip = landmarks[12]
    middle_pip = landmarks[10]
    
    ring_tip = landmarks[16]
    ring_pip = landmarks[14]
    
    pinky_tip = landmarks[20]
    pinky_pip = landmarks[18]
    
    # Count extended fingers
    fingers_up = []
    
    # Thumb (different logic for thumb)
    if thumb_tip.x > thumb_ip.x:  # Right hand
        fingers_up.append(thumb_tip.x > thumb_ip.x)
    else:  # Left hand
        fingers_up.append(thumb_tip.x < thumb_ip.x)
    
    # Other fingers
    fingers_up.append(index_tip.y < index_pip.y)
    fingers_up.append(middle_tip.y < middle_pip.y)
    fingers_up.append(ring_tip.y < ring_pip.y)
    fingers_up.append(pinky_tip.y < pinky_pip.y)
    
    total_fingers = sum(fingers_up)
    
    # Gesture recognition
    if total_fingers == 0:
        return "Fist", (0, 0, 255)  # Red
    elif total_fingers == 1 and fingers_up[1]:  # Only index finger
        return "Pointing", (255, 255, 0)  # Cyan
    elif total_fingers == 2 and fingers_up[1] and fingers_up[2]:  # Index and middle
        return "Peace/Victory", (0, 255, 255)  # Yellow
    elif total_fingers == 2 and fingers_up[0] and fingers_up[1]:  # Thumb and index
        return "Gun/L-Shape", (255, 0, 255)  # Magenta
    elif total_fingers == 3 and fingers_up[1] and fingers_up[2] and fingers_up[3]:
        return "Three", (0, 165, 255)  # Orange
    elif total_fingers == 4 and not fingers_up[0]:  # Four fingers, no thumb
        return "Four", (255, 165, 0)  # Orange
    elif total_fingers == 5:
        return "Open Hand/Hi", (0, 255, 0)  # Green
    elif fingers_up[0] and fingers_up[4] and total_fingers == 2:  # Thumb and pinky
        return "Rock On/Call Me", (128, 0, 128)  # Purple
    else:
        return f"{total_fingers} Fingers", (255, 255, 255)  # White

def detect_emotion(face_gray, face_color, eye_cascade, smile_cascade):
    """Detect emotion based on facial features and analysis"""
    
    # Detect features
    eyes = eye_cascade.detectMultiScale(face_gray, 1.1, 3)
    smiles = smile_cascade.detectMultiScale(face_gray, 1.8, 20)
    
    # Advanced analysis
    brightness = np.mean(face_gray)
    contrast = np.std(face_gray)
    
    # Analyze face regions
    h, w = face_gray.shape
    upper_face = face_gray[0:h//2, :]  # Forehead/eyes area
    lower_face = face_gray[h//2:, :]   # Mouth/chin area
    
    upper_brightness = np.mean(upper_face)
    lower_brightness = np.mean(lower_face)
    brightness_diff = abs(upper_brightness - lower_brightness)
    
    # Eye analysis
    num_eyes = len(eyes)
    eye_area_total = sum([w*h for (x,y,w,h) in eyes]) if len(eyes) > 0 else 0
    
    # Smile analysis
    num_smiles = len(smiles)
    smile_area_total = sum([w*h for (x,y,w,h) in smiles]) if len(smiles) > 0 else 0
    
    # Mouth region analysis (lower third of face)
    mouth_region = face_gray[2*h//3:, w//4:3*w//4]
    mouth_brightness = np.mean(mouth_region)
    mouth_contrast = np.std(mouth_region)
    
    # Edge detection for expression lines
    edges = cv2.Canny(face_gray, 50, 150)
    edge_density = np.sum(edges) / (w * h)
    
    # Emotion detection
    if num_smiles > 0:
        emotion = "Happy"
        color = (0, 255, 0)  # Green
        confidence = min(0.9, 0.6 + (num_smiles * 0.15) + (smile_area_total / 3000))
    elif num_eyes == 0 or (num_eyes < 2 and brightness < 100):
        emotion = "Sleepy"
        color = (255, 100, 100)  # Light Blue
        confidence = 0.4
    elif edge_density > 0.12 and (brightness_diff > 25 or contrast > 45):
        emotion = "Angry"
        color = (0, 0, 255)  # Red
        confidence = 0.8
    elif brightness < 95 or mouth_brightness < 85:
        emotion = "Sad"
        color = (255, 0, 0)  # Blue
        confidence = 0.7
    else:
        emotion = "Happy"
        color = (0, 200, 0)  # Light Green
        confidence = 0.6
    
    # Draw detected features
    for (ex, ey, ew, eh) in eyes:
        cv2.rectangle(face_color, (ex, ey), (ex+ew, ey+eh), (255, 0, 0), 1)
    
    for (sx, sy, sw, sh) in smiles:
        cv2.rectangle(face_color, (sx, sy), (sx+sw, sy+sh), (0, 255, 0), 1)
    
    return emotion, color, confidence

def main():
    print("=== Advanced Face & Hand Recognition System ===")
    print("Initializing...")
    
    # Test camera access
    camera_index = test_camera_access()
    if camera_index is None:
        print("\nâŒ CAMERA ERROR: No working camera found!")
        print("\nTroubleshooting steps:")
        print("1. Make sure your camera is not being used by another application")
        print("2. Check camera permissions for this application")
        print("3. Try disconnecting and reconnecting external cameras")
        print("4. Restart your computer if the issue persists")
        return
    
    # Initialize cascade classifiers
    face_cascade, eye_cascade, smile_cascade = initialize_cascades()
    if face_cascade is None:
        print("âŒ CASCADE ERROR: Could not load required files!")
        return
    
    # Initialize MediaPipe
    hands, mp_hands, mp_drawing = initialize_mediapipe()
    if hands is None:
        print("âŒ MEDIAPIPE ERROR: Could not initialize hand tracking!")
        print("Please install MediaPipe: pip install mediapipe")
        return
    
    # Initialize camera with found working index
    print(f"\nðŸŽ¥ Initializing camera {camera_index}...")
    cap = cv2.VideoCapture(camera_index)
    
    # Set camera properties for better performance
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
    cap.set(cv2.CAP_PROP_FPS, 30)
    
    # Verify camera opened successfully
    if not cap.isOpened():
        print("âŒ CAMERA ERROR: Failed to open camera!")
        return
    
    print("âœ… Camera initialized successfully!")
    print("\n=== Instructions ===")
    print("FACIAL EMOTIONS:")
    print("- Smile for 'Happy'")
    print("- Frown or look down for 'Sad'")  
    print("- Make angry face or furrow brows for 'Angry'")
    print("- Close eyes or look tired for 'Sleepy'")
    print("\nHAND GESTURES:")
    print("- Fist: Closed hand")
    print("- Pointing: Index finger up")
    print("- Peace: Index and middle finger up")
    print("- Gun/L-Shape: Thumb and index finger")
    print("- Open Hand: All fingers extended")
    print("- Rock On: Thumb and pinky up")
    print("\nControls:")
    print("Press 'q' to quit")
    print("Press 's' to save screenshot")
    print("Press 'h' to toggle hand detection")
    
    # Give camera time to warm up
    time.sleep(2)
    
    frame_count = 0
    show_hands = True
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("Warning: Failed to read frame")
                continue
            
            frame_count += 1
            
            # Convert to grayscale for face detection
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # Convert BGR to RGB for MediaPipe
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # Face Detection
            faces = face_cascade.detectMultiScale(gray, 1.1, 4, minSize=(100, 100))
            
            # Hand Detection
            hand_results = hands.process(rgb_frame) if show_hands else None
            
            # Process faces
            for (x, y, w, h) in faces:
                # Extract face regions
                face_gray = gray[y:y+h, x:x+w]
                face_color = frame[y:y+h, x:x+w]
                
                # Detect emotion
                emotion, color, confidence = detect_emotion(face_gray, face_color, eye_cascade, smile_cascade)
                
                # Draw face rectangle
                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 3)
                
                # Draw emotion label
                label = f"Face: {emotion} ({confidence:.0%})"
                cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
            
            # Process hands
            gesture_text = "No Hands Detected"
            gesture_color = (128, 128, 128)
            
            if show_hands and hand_results and hand_results.multi_hand_landmarks:
                for idx, hand_landmarks in enumerate(hand_results.multi_hand_landmarks):
                    # Draw hand landmarks
                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
                    
                    # Detect gesture
                    gesture, gesture_color = detect_hand_gesture(hand_landmarks.landmark)
                    
                    # Get hand bounding box
                    h, w, c = frame.shape
                    x_coords = [landmark.x * w for landmark in hand_landmarks.landmark]
                    y_coords = [landmark.y * h for landmark in hand_landmarks.landmark]
                    
                    x_min, x_max = int(min(x_coords)), int(max(x_coords))
                    y_min, y_max = int(min(y_coords)), int(max(y_coords))
                    
                    # Draw gesture label
                    gesture_label = f"Hand {idx+1}: {gesture}"
                    cv2.putText(frame, gesture_label, (x_min, y_min-10), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, gesture_color, 2)
                    
                    # Draw bounding box
                    cv2.rectangle(frame, (x_min-10, y_min-10), (x_max+10, y_max+10), gesture_color, 2)
                    
                    gesture_text = gesture
            
            # Status display
            status_y = 30
            cv2.putText(frame, f"Faces: {len(faces)}", (10, status_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            if show_hands:
                num_hands = len(hand_results.multi_hand_landmarks) if hand_results and hand_results.multi_hand_landmarks else 0
                cv2.putText(frame, f"Hands: {num_hands}", (10, status_y + 25), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                cv2.putText(frame, f"Gesture: {gesture_text}", (10, status_y + 50), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, gesture_color, 2)
            else:
                cv2.putText(frame, "Hand Detection: OFF", (10, status_y + 25), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
            
            # Instructions
            cv2.putText(frame, "q:quit | s:save | h:toggle hands", (10, frame.shape[0] - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            # Show FPS counter
            if frame_count % 30 == 0:
                cv2.putText(frame, f"Frame: {frame_count}", (frame.shape[1] - 150, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            cv2.imshow('Face & Hand Recognition System', frame)
            
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                print("Exiting...")
                break
            elif key == ord('s'):
                filename = f"recognition_capture_{int(time.time())}.jpg"
                cv2.imwrite(filename, frame)
                print(f"Screenshot saved: {filename}")
            elif key == ord('h'):
                show_hands = not show_hands
                print(f"Hand detection: {'ON' if show_hands else 'OFF'}")
    
    except KeyboardInterrupt:
        print("\nProgram interrupted by user")
    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        cap.release()
        cv2.destroyAllWindows()
        if hands:
            hands.close()
        print("Camera released and windows closed")

if __name__ == "__main__":
    main()
